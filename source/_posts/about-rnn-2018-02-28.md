title: about-RNN
date: 2018-02-28 01:32:28
mathjax: true
tags:

---

人的思维是具有连续性的，但是普通的神经网络并不不能维持记忆。循环神经网络的主要用途是**处理**和**预测** **序列数据**。

序列是一个比较抽象的概念，比如

* 股票价格曲线对应的时间轴
* 正弦曲线的x轴
* 一张图片的以宽度作为滑动窗口的序列
* 一段文字
* 一段语音

处理：即将指定维度的特征经过神经网络处理之后，再处理为原来的维度
预测：使用神经网络的输出直接做预测

## RNN

RNNs看成是一个普通的网络做了多次复制之后叠加在一起

![](/images/about-rnn/RNNs展开网络结构.png)


![普通 RNNs 内部结构](/images/about-rnn/普通的RNNs内部结构.png)

* RNN内部状态通过向量表示，维度h
* 输入向量维度 x
* 输入：上一个时刻的状态+当前时刻的输入，h+x
* 输出：输出为当前时刻的状态，维度h。即做一次 tanh(h+x->x 的全连接)


还是很抽象，看一下前向传播的具体过程普通

![](/images/about-rnn/循环神经网络前向传播计算过程.jpg)


RNN出现的主要目的是将以前的信息联系到现在，从而解决现在的问题。

但是随着预测信息和相关信息间的间隔增大， RNNs 很难去把它们关联起来了。

## LSTM 长短期记忆网络（Long Short Term Memory networks） 

![](/images/about-rnn/LSTM内部结构.png)

### 遗忘门

![](/images/about-rnn/遗忘门.png)

### 传入门

![](/images/about-rnn/传入门.png)


### 更新门

![](/images/about-rnn/更新cell状态.png)

## 其他变种

1. 双向神经网络 bidirectional RNN，两个单向神经网络的结合，比如 完形填空，需要预测的值和前后都相关
1. 深层神经网络，增强模型表达能力，在每一个时刻上将循环体重复多次，内部结构维度一样，但是值不一样


## Refercnce

[（译）理解 LSTM 网络 （Understanding LSTM Networks by colah）](http://blog.csdn.net/jerr__y/article/details/58598296)

